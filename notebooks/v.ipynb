{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-29T15:21:07.745948Z",
     "start_time": "2024-12-29T15:20:58.157667Z"
    }
   },
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "def load_model_and_tokenizer(model_name, device=\"cuda\"):\n",
    "    print(f\"Loading model: {model_name}\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    return model, tokenizer"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fzkuj\\anaconda3\\envs\\nano\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T15:21:41.342346Z",
     "start_time": "2024-12-29T15:21:07.755001Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 加载模型和 tokenizer\n",
    "model, tokenizer = load_model_and_tokenizer(model_name, device)\n",
    "\n",
    "# 加载和分词输入\n",
    "dataset = load_dataset(\"emozilla/pg19\", split=\"test\", trust_remote_code=True)\n",
    "text = dataset[0][\"text\"][:16384]  # 仅取前 16384 个字符\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\", max_length=16384, truncation=True).input_ids\n"
   ],
   "id": "a68fa0440578ca16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mistralai/Mistral-7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:21<00:00, 10.56s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T15:21:41.856865Z",
     "start_time": "2024-12-29T15:21:41.851101Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(text[:100])\n",
    "print(input_ids[:100])"
   ],
   "id": "30ed786303dab03a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ST. PAUL***\n",
      "\n",
      "\n",
      "E-text prepared by Josephine Paolucci and the Project Gutenberg Online\n",
      "Distributed Pro\n",
      "tensor([[    1,   920, 28723,  ..., 28725,  3364, 28705]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-12-29T11:07:06.823044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 提取注意力分数\n",
    "# 模型前向\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids.to(device), output_attentions=True)"
   ],
   "id": "241cf83bdbfc0246",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: mistralai/Mistral-7B-Instruct-v0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fzkuj\\anaconda3\\envs\\nano\\Lib\\site-packages\\accelerate\\utils\\modeling.py:1390: UserWarning: Current model requires 4096 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:24<00:00, 12.39s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "Using the latest cached version of the dataset since emozilla/pg19 couldn't be found on the Hugging Face Hub\n",
      "Found the latest cached dataset configuration 'default' at C:\\Users\\fzkuj\\.cache\\huggingface\\datasets\\emozilla___pg19\\default\\0.0.0\\c021754c8e01c5b1cc83a1f549c1f97fbbb756b8 (last modified on Sat Dec 28 15:02:36 2024).\n",
      "MistralModel is using MistralSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-29T11:05:11.455421Z",
     "start_time": "2024-12-29T11:05:11.450641Z"
    }
   },
   "cell_type": "code",
   "source": "print(output.keys())",
   "id": "a4176d25b06cb8a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "output[\"attentions\"]",
   "id": "75b41a4cd84f22df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "# 可视化某一层的注意力分数\n",
    "def visualize_attention_scores(attention_scores, layer_idx, head_idx, seq_len, output_file=None):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    attention_matrix = attention_scores[layer_idx][0, head_idx, :seq_len, :seq_len]\n",
    "    plt.imshow(attention_matrix, cmap=\"viridis\", aspect=\"auto\")\n",
    "    plt.colorbar(label=\"Attention Score\")\n",
    "    plt.title(f\"Attention Layer {layer_idx + 1}, Head {head_idx + 1}\")\n",
    "    plt.xlabel(\"Key Position\")\n",
    "    plt.ylabel(\"Query Position\")\n",
    "    if output_file:\n",
    "        plt.savefig(output_file)\n",
    "    plt.show()\n"
   ],
   "id": "4ad947e8d99977da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "attention_scores = extract_attention_scores(model, input_ids, device)\n",
    "\n",
    "# 保存或可视化\n",
    "layer_idx = 0  # 可视化第几层\n",
    "head_idx = 0   # 可视化第几个注意力头\n",
    "seq_len = 1024  # 只显示前 1024 长度的分数（避免图太大）\n",
    "visualize_attention_scores(attention_scores, layer_idx, head_idx, seq_len, output_file=\"attention_layer_1_head_1.png\")\n",
    "\n",
    "# 保存完整注意力分数到文件\n",
    "torch.save(attention_scores, \"attention_scores.pt\")\n",
    "print(\"Attention scores saved to 'attention_scores.pt'\")"
   ],
   "id": "add03ca42b8c221"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
